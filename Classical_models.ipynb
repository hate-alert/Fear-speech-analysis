{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T09:07:16.300209Z",
     "start_time": "2020-09-26T09:07:16.126647Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from laserembeddings import Laser\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T08:09:35.033572Z",
     "start_time": "2020-09-26T08:09:35.027686Z"
    }
   },
   "outputs": [],
   "source": [
    "parent_path='../Data/New_Data_15-06-2020/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4782\n"
     ]
    }
   ],
   "source": [
    "annotated_df=pd.read_pickle(parent_path+'Fearspeech_data_final.pkl')\n",
    "# annotated_df=remove_duplicates_within(annotated_df)\n",
    "print(len(annotated_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = []\n",
    "lang_text = []\n",
    "for index,row in tqdm(annotated_df.iterrows(),total=len(annotated_df)):\n",
    "    if len(row['message_text'])>4000:\n",
    "        string=row['message_text'][0:2000]+row['message_text'][-2000:]\n",
    "        list_text.append(demoji.replace(string,repl=\"\"))\n",
    "    else:\n",
    "        list_text.append(demoji.replace(row[\"message_text\"],repl=\"\"))\n",
    "    lang_text.append(row[\"language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser = Laser()\n",
    "\n",
    "embed_list=[]\n",
    "\n",
    "length_given=len(list_text)\n",
    "batch_size=64\n",
    "for i in tqdm(range(0,length_given,batch_size)):\n",
    "    if(i+batch_size<=length_given):\n",
    "        temp_text=list_text[i:i+batch_size]\n",
    "        temp_lang=lang_text[i:i+batch_size]\n",
    "    else:\n",
    "        temp_text=list_text[i:length_given]\n",
    "        temp_lang=lang_text[i:length_given]\n",
    "    embeddings = laser.embed_sentences(temp_text,lang=temp_lang)  # lang is only used for tokenization\n",
    "    embed_list+=list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-526b7021bc4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'one_fear_speech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embed_list' is not defined"
     ]
    }
   ],
   "source": [
    "X_0 = np.array(embed_list)\n",
    "y_0 = np.array(annotated_df['one_fear_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix,make_scorer, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score,classification_report, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "from tqdm import tqdm_notebook\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#export\n",
    "def extract_emojis(str1):\n",
    "    try:\n",
    "        return [c for c in str1 if c in emoji.UNICODE_EMOJI]\n",
    "    except AttributeError:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [446 447 448 449 450] TEST: [0 1 2 3 4]\n",
      "{0: 0.6567460317460317, 1: 2.0949367088607596}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3138bd222a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#classifier=SVC(class_weight=class_weights,kernel='rbf',probability=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "acc=[]\n",
    "macro_f1=[]\n",
    "prec=[]\n",
    "recall=[]\n",
    "prob=[]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state= 2020)\n",
    "\n",
    "for train_index, test_index in skf.split(X_0, y_0):\n",
    "    print(\"TRAIN:\", train_index[0:5], \"TEST:\", test_index[0:5])\n",
    "    X_train, X_test = X_0[train_index], X_0[test_index]\n",
    "    y_train, y_test = y_0[train_index], y_0[test_index]\n",
    "    class_weights = dict(zip(np.unique(y_train), compute_class_weight(\"balanced\", np.unique(y_train),y_train)))\n",
    "    print(class_weights)\n",
    "    classifier= LogisticRegression(class_weight=class_weights)\n",
    "    #classifier=SVC(class_weight=class_weights,kernel='rbf',probability=True)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred=classifier.predict(X_test)\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    macro_f1.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    prec.append(precision_score(y_test, y_pred))\n",
    "    recall.append(recall_score(y_test, y_pred))\n",
    "    prob.append(classifier.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For SVC (with rbf kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For logisitic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72 (+/- 0.09)\n",
      "Macro F1: 0.66 (+/- 0.08)\n",
      "Precision for +ve class: 0.45 (+/- 0.16)\n",
      "Recall for +ve class: 0.59 (+/- 0.13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4782/4782 [01:03<00:00, 75.11it/s] \n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import *\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "list_sents=[preprocess_doc(ele,params={'remove_numbers': True, 'remove_emoji': True, 'remove_stop_words': False, 'tokenize': True}) for ele in tqdm(annotated_df['message_text'],total=len(annotated_df))]\n",
    "#list_emojis = [\" \".join(extract_emojis(ele)) for ele in tqdm(annotated_df['message_text'],total=len(annotated_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sents=[1,2,3,4]\n",
    "list_sents[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = np.array(list_sents,dtype='object')\n",
    "X_1 = np.array(list_emojis,dtype='object')\n",
    "y_0 = np.array(annotated_df['one_fear_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3= np.vstack((X_0, X_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3=np.transpose(X_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punyajoy/.conda/envs/punyajoy_gpu/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [870 872 873 874 875] TEST: [0 1 2 3 4]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "(3825, 63) (3825, 300)\n",
      "TRAIN: [0 1 2 3 4] TEST: [870 872 873 874 875]\n",
      "{0: 0.6567651098901099, 1: 2.0947426067907995}\n",
      "(3825, 54) (3825, 300)\n",
      "TRAIN: [0 1 2 3 4] TEST: [1771 1772 1773 1774 1775]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "(3826, 60) (3826, 300)\n",
      "TRAIN: [0 1 2 3 4] TEST: [2757 2758 2759 2761 2762]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "(3826, 60) (3826, 300)\n",
      "TRAIN: [0 1 2 3 4] TEST: [3792 3794 3796 3799 3801]\n",
      "{0: 0.6569368131868132, 1: 2.0929978118161925}\n",
      "(3826, 67) (3826, 300)\n"
     ]
    }
   ],
   "source": [
    "acc=[]\n",
    "macro_f1=[]\n",
    "prec=[]\n",
    "recall=[]\n",
    "prob=[]\n",
    "auc_roc=[]\n",
    "list_total_preds=[]\n",
    "list_total_truth=[]\n",
    "skf = StratifiedKFold(n_splits=5, random_state= 2020)\n",
    "\n",
    "for train_index, test_index in skf.split(X_3, y_0):\n",
    "    print(\"TRAIN:\", train_index[0:5], \"TEST:\", test_index[0:5])\n",
    "    X_train, X_test = X_3[train_index], X_3[test_index]\n",
    "    y_train, y_test = y_0[train_index], y_0[test_index]\n",
    "    \n",
    "    \n",
    "    class_weights = dict(zip(np.unique(y_train), compute_class_weight(\"balanced\", np.unique(y_train),y_train)))\n",
    "    \n",
    "    \n",
    "    print(class_weights)\n",
    "    ### Generate doc2vec vectors\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train[:,0])]\n",
    "    model = Doc2Vec(documents, vector_size=300, window=5, min_count=1, workers=10)\n",
    "    X_train_embed = np.array([list(model.infer_vector(ele)) for ele in X_train[:,0]])\n",
    "    X_test_embed = np.array([list(model.infer_vector(ele)) for ele in X_test[:,0]])\n",
    "    \n",
    "    ##generate emoji vectors\n",
    "    tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),min_df=0.01,max_df=0.95,analyzer='char')\n",
    "\n",
    "    X_train_tfidf_emoji = tf_idf_vec_smooth.fit_transform(X_train[:,1]).toarray()\n",
    "    X_test_tfidf_emoji = tf_idf_vec_smooth.transform(X_test[:,1]).toarray()\n",
    "    \n",
    "    print(X_train_tfidf_emoji.shape,X_train_embed.shape)\n",
    "    \n",
    "    X_train_embed_2= np.concatenate((X_train_embed, X_train_tfidf_emoji),axis=1)\n",
    "    X_test_embed_2= np.concatenate((X_test_embed, X_test_tfidf_emoji),axis=1)\n",
    "    \n",
    "    \n",
    "    classifier= LogisticRegression(class_weight='balanced',max_iter=500)\n",
    "    #classifier=SVC(class_weight='balanced',kernel='rbf',probability=True)\n",
    "    classifier.fit(X_train_embed_2, y_train)\n",
    "    y_pred=classifier.predict(X_test_embed_2)\n",
    "    y_pred_proba = classifier.predict_proba(X_test_embed_2)\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    macro_f1.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    auc_roc.append(roc_auc_score(y_test, y_pred_proba[:,1],average='macro'))\n",
    "    prec.append(precision_score(y_test, y_pred))\n",
    "    recall.append(recall_score(y_test, y_pred))\n",
    "    prob.append(classifier.predict_proba(X_test_embed_2))\n",
    "    list_total_preds+=list(y_pred)\n",
    "    list_total_truth+=list(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76 (+/- 0.05)\n",
      "Macro F1: 0.69 (+/- 0.04)\n",
      "Auc Roc F1: 0.78 (+/- 0.06)\n",
      "Precision for +ve class: 0.50 (+/- 0.12)\n",
      "Recall for +ve class: 0.62 (+/- 0.13)\n",
      "             precision    recall  f1-score  support  accuracy\n",
      "0             0.869928  0.801099  0.834096   3640.0  0.801099\n",
      "1             0.493706  0.618214  0.548989   1142.0  0.618214\n",
      "avg / total   0.681817  0.709656  0.691543   4782.0  0.757424\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Auc Roc F1: %0.2f (+/- %0.2f)\" % (np.mean(auc_roc), np.std(auc_roc) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))\n",
    "print(pandas_classification_report(list_total_truth, list_total_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72 (+/- 0.08)\n",
      "Macro F1: 0.66 (+/- 0.06)\n",
      "Auc Roc: 0.76 (+/- 0.06)\n",
      "Precision for +ve class: 0.45 (+/- 0.13)\n",
      "Recall for +ve class: 0.62 (+/- 0.11)\n",
      "             precision    recall  f1-score  support  accuracy\n",
      "0             0.862975  0.749176  0.802059   3640.0  0.749176\n",
      "1             0.437115  0.620841  0.513025   1142.0  0.620841\n",
      "avg / total   0.650045  0.685008  0.657542   4782.0  0.718528\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(acc), np.std(acc) * 2))\n",
    "print(\"Macro F1: %0.2f (+/- %0.2f)\" % (np.mean(macro_f1), np.std(macro_f1) * 2))\n",
    "print(\"Auc Roc: %0.2f (+/- %0.2f)\" % (np.mean(auc_roc), np.std(auc_roc) * 2))\n",
    "print(\"Precision for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(prec), np.std(prec) * 2))\n",
    "print(\"Recall for +ve class: %0.2f (+/- %0.2f)\" % (np.mean(recall), np.std(recall) * 2))\n",
    "print(pandas_classification_report(list_total_truth, list_total_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4782\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "annotated_df=pd.read_pickle(parent_path+'Fearspeech_data_final.pkl')\n",
    "# annotated_df=remove_duplicates_within(annotated_df)\n",
    "print(len(annotated_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_sents=[]\n",
    "for index,row in annotated_df.iterrows():\n",
    "    str1=\" \".join(extract_emojis(row['message_text']))\n",
    "    emoji_sents.append(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Smoothing:\n",
      "                       ðŸ‡®         ðŸ‡³    ðŸ»    ðŸ‘‡         ðŸ™         ðŸš©\n",
      "0     0.962035  0.050357  0.050217  0.0  0.0  0.045789  0.259488\n",
      "1     0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "2     0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "3     0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "4     0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "...        ...       ...       ...  ...  ...       ...       ...\n",
      "4777  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "4778  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "4779  0.592966  0.362112  0.361106  0.0  0.0  0.000000  0.621988\n",
      "4780  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "4781  0.000000  0.000000  0.000000  0.0  0.0  0.000000  0.000000\n",
      "\n",
      "[4782 rows x 7 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-punyajoy_gpu] *",
   "language": "python",
   "name": "conda-env-.conda-punyajoy_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
